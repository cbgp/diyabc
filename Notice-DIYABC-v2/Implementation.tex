\clearpage
 \section{Implementation details}
\subsection{Software design}
$DIYABC$ v2 has been designed in a very different way compared to version 1. Version 1 was a single executable file were the GUI \footnote{Graphic User Interface} and computation codes were highly intricated and both written in the same language (\emph{Delphi}). In version 2, the GUI and the computation codes have been completely separated. Actually, the GUI is a script written in \emph{python} and all computations are included in a program written in \emph{C++}. In opposition to \emph{Delphi} which is restricted to a single OS (\texttt{Windows}), \textit{python} and \textit{C++} can be used with the main three OS (\texttt{Linux}, \texttt{Mac} and \texttt{Windows}), allowing version 2  to be operated under all three OS.\\
The GUI uses the \textit{Qt} graphic library. The computation code is linked to the \textit{openmp} library allowing a better use of multicore/multiprocessor computers.\\
The GUI can launch the computation program with the right parameters and keeps track of the progress of the latter through small log files. The GUI can launch as many computation programs as there are open projects, but no more than one computation program per project. A \textit{lock} file located in the project directory is created when the computation program is launched by the GUI and removed when the computation program has normally terminated. When the computation program has exited anormaly, the GUI issues an error message trying to explain where the programm failed.    
\subsection{Files}
The program uses and produces various files which we will describe now.
\subsubsection{data files}
Data files are text files that contain information about the samples : number and names of microsatellite markers, multilocus genotypes of individuals. The basic  format is that of the Genepop software \citep{RR1995} and data files produced by DIYABC are under this format.  \textbf{Microsatellite genotypes must be noted with 3 (haploid) or 6 (diploid) digits, these three digit numbers being the length in nucleotides of the corresponding PCR products}. In addition, we have added some features to this basic format in order to use sequence data. All these additions are explained in section 4.4. SNP data correspond to a different file format, also detailed in section 4.4.\\ 
Any extension is accepted for datafile names, including no extension at all. If the data file is simulated with $DIYABC$, the extension is \texttt{mss} for microsatellite/DNA sequence data and \texttt{snp} for SNP data. The next page shows examples of data sets saved.

\subsubsection{reference table files} 
Reference table files are binary files which include two successive parts :
\begin{itemize}
 \item The first part is a header which contains information necessary to read the second part, such as the number of scenarios, or the number of parameters of each scenario.
 \item The second part contains simulated data set records, each record containing the scenario number, the parameter and summary statistics values.
\end{itemize}
 
 Each time a reference table is created or increased (each time the \fbox{\textsf{Run computation}} button is pressed), a text file is created in the project directory with the name \texttt{first\_records\_of\_the\_reference\_table\_X.txt} in which \texttt{X} is an integer number starting at 0 and increasing each time the \fbox{\textsf{Run computation}} button is pressed. This file provides a text version of the first $n$ newly created records of the reference table ($n$ being equal to the \emph{Particle loop size}, see section 3.7.3).\\

\subsubsection{output files}
As already seen, DIYABC achieves different analyses : comparison of scenarios, estimation of posterior distribution of parameters, model checking,  computation of bias and mean square errors and evaluation of confidence in scenario choice. Each analysis has its own output which can be printed and saved. Graphs are saved under the chosen format and non-graphic output are saved in text files.\\

We now describe all the files produced by each type of analysis. These files are located in directories (one directory per analysis) gathered in the \texttt{analysis} subdirectory of the project directory. Below is an example of the \texttt{TOYTEST2\_2012\_9\_26-1} project directory substructure:\\

\includegraphics[scale=0.5]{gui_pictures/Capture-DIYABC-103.png}\\

Note that each directory name starts with the name of analysis followed by the type of analysis, $e.g.$ \texttt{bias} for a bias/precision analysis or \texttt{comparison} for a comparison of scenarios. In addition, when a picture has been saved, the corresponding file is located under a subdirectory named \texttt{pictures} ($e.g.$ at the bottom of the figure above).
\begin{description}
 \item [Pre-evaluate scenario prior combinations :] This analysis can produce two output files named \texttt{ACP.txt} and \texttt{locate.txt}. The former is the output of the Principal Component Analysis and the latter that of the analysis giving the proportion of simulated data sets which have a value below the observed value for every summary statistics. This latter file is exactly what appears in the GUI. The structure of the  \texttt{ACP.txt} file is the following. The first line indicates the number of points of the PCA, the number of PCA components (axes) and the inertia of each component, all values are separated by a single space. The second line provides the components of the observed data. It starts with a zero which corresponds to the scenario number in the following lines. Each subsequent line provides the components of data simulated according to a given scenario which number is at the beginning of the line. If one or more PCA figures have been saved, the corresponding files are saved in the \texttt{pictures} subdirectory. They are named as \texttt{refTable\_PCA\_X\_Y\_N.pdf}, with \texttt{X} and  \texttt{Y} giving the axis numbers and \texttt{N} being the number of represented points. 
 \item [Compute posterior probabilities of scenarios :] This analysis produces three output text files : \texttt{compdirect.txt}, \texttt{complogreg.txt} and \texttt{compdirlog.txt}. The latter is directly visualized in the GUI when clicking the \fbox{\textsf{view numerical results}} button. The first two files are used by the GUI to elaborate the two graphics (Direct approach and Logistic regression). Again, if graphics have been saved, the corresponding file(s) is(are) in the \texttt{pictures} subdirectory of the analysis directory.  
 \item [Evaluate confidence in scenario choice :] This analysis produces a single output file, \texttt{confidence.txt}, the content of which is visualized in the GUI.
 \item [Estimate posterior distributions of parameter :] Nine files are written as output of this type of analysis :
  \begin{itemize}
   \item three files \texttt{mmmq\_original.txt}, \texttt{mmmq\_composite.txt} and \texttt{mmmq\_scaled.txt} contain the statistics (mean, median, mode and quantiles) for the original, composite and scaled parameters, respectively. They are visualized in the GUI when clicking the \fbox{\textsf{view numerical results}} button. 
   \item three files \texttt{paramstatdens\_original.txt}, \texttt{paramstatdens\_composite.txt} and \texttt{paramstatdens\_scaled.txt} are used by the GUI to produce the graphics showing prior/posterior distribution.
   \item three files \texttt{phistar\_original.txt}, \texttt{phistar\_composite.txt} and \texttt{phistar\_scaled.txt} contains the $\phi^*$ values of the original, composite and scaled parameters, respectively. These files can be used for instance to redraw posterior distributions, $e.g.$ with the $R$ software.
  \end{itemize}
As already mentionned, saved graphics are located in a \texttt{pictures} subdirectory.
 \item [Compute bias and precision of parameter estimations :] Three files \texttt{bias\_original.txt}, \texttt{bias\_composite.txt} and \texttt{bias\_scaled.txt} are produced by this type of analysis. All three files are visualized in the GUI. 
 \item [Perform model-checking] The output files of this type of analysis are the same as those of the \textit{Pre-evaluate scenario prior combinations} analysis (see above). The only difference is in the names of the two text files which start with \texttt{mc} for \texttt{model checking}. 
\end{description}
~\\

In addition, the GUI program writes several files in the project directory :
\begin{description}
 \item [command.txt :] this text file contains the history of commands issued by the GUI to be achieved by the computation program.
 \item [conf.analysis :] this text file contains information about analyses.
 \item [conf.gen.tmp :] this text file contains information about the loci, the genetic parameters and the summary statistics.
 \item [conf.hist.tmp :] this text file contains information about the scenario and the historical parameters.
 \item [conf.th.tmp :] This text file contains the title line of the reference table.
 \item [conf.tmp :] This text file contains the name of the dataset and the number of parameters and summary statistics.
 \item [header.txt :] This text file is a concatenation of the previous four files and is red by the computation program.
 \item [xxx.diyabcproject :] This text file contains the path to the \texttt{xxx} project.
 \item [RNG\_state\_0000.bin :] This binary file contains the current state of the random generator.
 \item [init\_rng.out :] This text file contains information about the initialization of the random generator.
\end{description}
~\\

The computation program writes the following files in the project directory :
\begin{description}
 \item [reftable.log :] This text file is produced when a reftable is increased. It provides the GUI with information about the progress of computations : achieved number of records, time left.
 \item [statobs.txt :] This text file is written every time an analysis is performed. It contains the values of summary statistrics for the observed data set.
\end{description}

The following files are output by the computation program everytime it has been launched by a specific command of the GUI (their use is only for debugging purposes and they are all in the project directory) :
\begin{description}
 \item [general.out :] when computing a reftable.
 \item [pre-ev.out :] when performing a \textit{Pre-evaluate scenario prior combinations} analysis.
 \item [compare.out :] when performing a \textit{Compare scenarios} analysis.
 \item [confidence.out :] when performing a \textit{Confidence in scenario choice} analysis.
 \item [estimate.out :] when performing a \textit{ABC parameter estimation} analysis.
 \item [bias.out :] when performing a \textit{bias-precision} analysis.
 \item [modelChecking.out :] when performing a \textit{model checking} analysis.
\end{description}

When performing a \textit{Bias-precision} or a \textit{Confidence in scenario choice} analysis, the computation program simulates what we call \textit{pseudo-observed datasets}. The parameter and summary statistics values of these pseudo-observed datasets are written in a text file named \textbf{pseudo-observed\_datasets\_xxx.txt} in which \textbf{xxx} is the name given to the analysis.


\subsection{Missing data}
Missing or undetermined genotypes should be coded as \texttt{000} (haploid microsatellites), \texttt{000000}  (diploid microsatellites), \texttt{$<[~]>$} (haploid sequences) or \texttt{$<[~][~]>$} (diploid sequences) and \texttt{9} (SNP) in the data file. \\
Missing data are taken into account in the following way. For each appearance of a missing genotype in the observed data set, the programs records the individual and the locus. When simulating data sets, the program replaces the simulated genotype (obtained through the coalescence process algorithm) by the missing data code at all corresponding locations. All summary statistics are thus computed with the same missing data as for the observed data set. 

\subsection{Data files}
There are two different incompatible formats for data files, one for SNP loci and the other for microsatellite/DNA sequence data.\\ For the microsatellite/DNA sequence data, the format already presented in version 1 of DIYABC is an extended Genepop format. The additional features are :
\begin{enumerate}
\item In the title line appears the sex ratio noted between \textsf{$<$} and \textsf{$>$} under the form \textsf{$<NM=rNF>$}, in which $r$ is the ratio of the number of females per male ($e.g.$ \textsf{$<NM=2.5NF>$} means that the number of males is 2.5 times the number of females). Since the title is generally only copied, this addition should not interfere with other programs using  Genepop datafiles. Also if there is no such sex ratio addition, DIYABC will consider by default that NM=NF.
\item After the locus name, there is an indication for the category of the locus which is $<A>$ for autosomal diploid loci, $<H>$ for autosomal haploid loci, $<X>$ for X-linked (or haplo-diploid) loci, $<Y>$ for Y-linked loci and $<M>$ for mitochondrial loci. If no category is noted, DIYABC will consider the locus as autosomal diploid or autosomal haploid depending on the corresponding genotype of the first typed individual.
\item Genotypes of microsatellite loci are noted with six digit numbers (e.g. 190188) if diploid and by three digit numbers (e.g. 190) if haploid.
\item Sequence locus are noted between  \textsf{$<$} and \textsf{$>$} . In addition each sequence allele is noted between brackets. For instance, a haploid sequence locus  will be noted $<[$GTCTA$]>$ and a diploid sequence locus $<[$GTCTA$][$GTCTT$]>$.
\item Missing microsatellite genotypes are noted \textsf{000} if haploid or \textsf{000000} if diploid.
\item Missing sequence genotypes are noted $<[\ ]>$ if haploid or $<[\ ][\ ]>$ if diploid.
\end{enumerate}

For SNP data, the format includes:
\begin{itemize}
 \item a first line providing the sex-ratio as above and any text that can be used as a title (the sex ratio can be anywhere in this line).
 \item a second line starting with the three keywords \texttt{IND  SEX  POP}, separated by at least one space, followed by as many letters as SNP loci, the letter giving the location of the locus as above ($<A>$ for autosomal diploid loci, $<H>$ for autosomal haploid loci, $<X>$ for X-linked (or haplo-diploid) loci, $<Y>$ for Y-linked loci and $<M>$ for mitochondrial loci). Letters are separated by a single space.
 \item as many lines as there are genotyped individuals, with the code-name of the individual, a letter ($M$ or $F$) indicating its sex, a code-name for its population and the values (0, 1 or 2) of the number of the (arbitrarily chosen) reference allele at each SNP locus. 0 = homozygous genotype for the non reference allele, 1 = heterozygous genotype for the reference allele, 2 = homozygous genotype for the reference allele.
\end{itemize}


 
Below are three examples of data sets that can be analyzed with DIYABC.\\ In the first example, this data set includes two population samples, each of 12 diploid individuals (8 females and 4 males in the first sample and 5 females and 7 males in the second sample). As deduced from the letter between $<$ and $>$ on the locus name lines (see page 25), these individuals have been genotyped at 3 microsatellite loci (1 autosomal $<A>$, 1 X-linked $<X>$ and 1 Y-linked $<Y>$) and 3 DNA sequence loci (1 autosomal. 1 X-linked and 1 mitochondrial $<M>$). The species sex-ratio, given in the title line, is of three males for one female ($<NM=3NF>$) or in other words, the number of males equals three times the number of females. 
\begin{figure}[h]
\includegraphics[scale=0.6]{gui_pictures/screenga001.png}
\end{figure}
\newpage
In the second example, the species is haploid. Individuals have been genotyped at three autosomal microsatellite loci and one mitochondrial DNA sequence locus. The species being haploid (deduced from the presence of autosomal haploid loci), no indication of the sex-ratio appears in the title line.

\begin{figure}[h]
\includegraphics[scale=0.6]{gui_pictures/screenga002.png}
\end{figure}

In the third example, the species is diploid and has been genotyped at a large number of SNP autosomal loci. The first line provides the title which includes the species sex-ratio. The second line indicates what's in the different columns : individual name in column 1, individual sex in column 2, population name in column 3 and one column per SNP locus (the letter \texttt{A} indicates that the autosomal locus is autosomal; it would be \texttt{X} for an X-linked locus, \texttt{Y} for a Y-linked locus and \texttt{M} for a mitochondrial locus) . Columns are sparated by one or more spaces. SNP are coded 0, 1 or 2 according to the number of reference alleles at the corresponding locus. Only the top left part of the data file is represented below :

\begin{figure}[h]
\includegraphics[scale=0.5]{gui_pictures/screenga003.png}
\end{figure}



\clearpage
\section{Cluster version}\label{cluster}
The ABC approach requires simulating data sets, which is a time consuming. Typically, one to several millions data sets are needed to build up a reference table and this process can last several hours to several days. To take advantage of a computer grid cluster, the user can activate the option on the cluster tab of the general settings. By checking \textsf{use a cluster}, the settings of the cluster tab are editable and, while this option remains checked, DIYABC will not compute any reftable on your computer. 

A reftable is generated in 3 sequential steps :
\begin{enumerate}
 \item configure the required parameters in the GUI frontend and generate the cluster bundle (set of zipped files)
 \item transfer the bundle to the cluster and run it
 \item transfer back the reference table and include it to the project
\end{enumerate}

\subsection{configure the required parameters in the GUI frontend and generate the cluster bundle}\label{clusterconfigure}
In the settings cluster tab, you can configure some parameters which will be included in a bash script (cf section 3.7.1). This bash script named \texttt{launch.sh} will be executed on the cluster (cf section \ref{clusterrun}). The bash script is artificially split in two parts in the interface.

\includegraphics[scale=0.33]{gui_pictures/Capture-DIYABC-cluster.png} \\


You have to understand and edit five parameters of the cluster settings. Each one will be used as a variable in the main script \texttt{launch.sh}.
 

\subsubsection{number of data sets by job}
\texttt{numSimulatedDatasetByJob} : This integer indicates the number of data sets produced by each single job. It represents the granularity of your computations on the cluster. It gives you the hand to optimize your cluster utilization, i.e. the larger \texttt{numSimulatedDatasetByJob}, the less jobs will be created that consume more time. This option enables to optimize your use of the cluster according to your access limitations. The number of jobs submitted on the cluster is given the total number of data sets divided by \texttt{numSimulatedDatasetByJob}. 

A few examples illustrate the importance of this parameter:
\begin{itemize}
 \item  if your cluster is overloaded and the waiting time in queue is long then it is preferable to choose a high amount of data sets by job (less jobs)
 \item if a queue for short jobs is free while the other ones are full then it is preferable to choose a low amount of data sets by job (more jobs) and submit to the short queue
\end{itemize}

\subsubsection{number of cores by job}
\texttt{coresPerJob} : This integer indicates how many CPU cores will be used by each job. Be carefull : if you use lots of cores, to generate the RNG states will be time consuming (fig \ref{fig:rngTimeGeneration}). Increasing the number of cores generally increases the queueing waiting time. Remember that a job with 40 cores will not run faster than 40 jobs with one core each. \\
DIYABC includes openMP. It means that DIYABC can use several cores in a single computer but as it does not include MPI it can not use many cores on many computers for one same job. So please be sure to use an apropriate parallele environment to submit your jobs. Ask to your cluster system administrator. 

\begin{figure}[htb]
\centering \it	
  \begin{tabular}{|  l || c | c | c | c | c | c | c | }
	   \hline
	   number of jobs (t) & \multicolumn{7}{c|}{number of cores (c)} \\
	          & 1 & 4   & 8     & 16 & 32 & 40 & 80 \\
	   \hline \hline
	   100 & 20''  & 1'20" & 3'  & 10'   & 20'   & 26'   & 30' \\
	   \hline
	   200 & 50" & 3'    & 7'  & 20'   & 40'   & 50'   & 1h10' \\
	   \hline
	   500 & 2'  & 8'    & 25' & 50'   & 1h40' & 2h06' & 2h45' \\
	   \hline
	   1000 & 4' & 16'   & 50' & 1h40' & 2h10' & 2h45' & \\
	   \hline
   \end{tabular}
  \caption[width=.6\textwidth]{\label{fig:rngTimeGeneration} \it\footnotesize
    \textbf{RNG files Time Generation.} As shown in section~\ref{rng}, one caveat of the RNG
    method is the obligate generation of all the RNG files at once (generating the RNG files one by one 
    for each job on a cluster is possible but will result in a dangerous bias). The second
    caveat of the RNG method is a consequence of the first one, ie the time needed to generate the RNG
    files increases depending on the number of RNG files \texttt{t} and the number of cores \texttt{c}
    available for each RNG file. Once a file is generated, it is not possible to add cores. }
\end{figure}

\subsubsection{the number of concurrent jobs}
\texttt{maxConcurrentJobs} : This integer indicates the maximum number of jobs allowed to run simultaneously on the cluster. Why is this information required ? One caveat of the RNG (section \ref{rng}) is the need to simultaneously generate all RNG files, before starting the jobs queue submission. The main script \texttt{launch.sh} will produce a pool of RNG files and the script \texttt{nodes.sh} running on each cluster node will randomly draw a RNG file not currently in use. Once again, you should ask your cluster system administrator to know the running jobs limitation on your cluster. You can also evaluate this number by looking at your cluster load. Be carefull : if you use lots of RNG files, to generate the RNG states is time consuming (fig \ref{fig:rngTimeGeneration}).

\subsubsection{the seed to generate RNG files}
\texttt{seed} : This integer indicates to DIYABC the seed to start producing RNG files. By wrinting \texttt{-1} you will ask to use a random seed (recommended). Using a user-defined seed is only for test or debbugging purposes. 

\subsubsection{The diyabc executable}
\texttt{diyabcPath} : This path is for the binary executable to be used on the cluster. There is to way of use it :
\begin{itemize}
 \item  By choosing \texttt{cluster}, you are invited to write the absolute path of a diyabc binary as it match to your cluster file architecture. By this way the script \texttt{node.sh} will know which diyabc cluster binary to use. Ask your cluster administrator in order to know this path. This absolut path need to be the same for all your cluster nodes. 
 \item  By choosing \texttt{local}, you are invited to write the path of a diyabc binary executable on your personal computer. This binary will be added to the bundle to be tranfered to your cluster
\end{itemize}

\subsubsection{main script : launch.sh}
The next two text boxes in the interface deal with the first and last parts of the main script \texttt{launch.sh}. \texttt{launch.sh} will generate a pool of RNG files, submit jobs to the scheduler queuing system using a \texttt{node.sh} script, monitor the jobs and pool all reftable files into a single reftable file when all jobs are completed.

\begin{description}
	\item[first part] This part is not editable as it includes the variables used by DIYABC GUI frontend and other variables from your project : \texttt{numSimulatedDataSet}, \texttt{projectName}, \texttt{dataFileName}. 
	
	\item[last part] This part deals with the jobs submission. By default, \texttt{launch.sh} targets a $Grid Engine$ cluster. You probably need to customise this script to fit your cluster configuration (scheduler system, queue name, ...). You should mainly need to modify the  \texttt{\#\#\#\#\# EDIT \#\#\#\#\#} section. Please ask for help to your cluster system administrator.
\end{description}

\subsection{transfer the bundle to the cluster and run it}\label{clusterrun}
Once you have checked the box \textsf{Use a cluster (...)}, configured the cluster parameters and the \texttt{launch.sh} script in the general settings and saved them, you need to click on the \fbox{\textsf{Run computation}} button from your project panel. Next, you will need to choose a name for a tar archive including all files necessary for the cluster run, and save it on your computer. In order to transfer the tar archive to your cluster account you will probably need an sftp client like FileZilla or WinSCP. Once the archive is stored on your cluster working directory, you will need to login on your cluster with a shell console and untar your archive typing :\\
%\fbox{
   \begin{minipage}{0.9\textwidth}
\begin{lstlisting}
tar -xvf <yourTarArchiveName.tar>
cd yourTarArchiveName

\end{lstlisting}
   \end{minipage}
%}

 This will create a directory with all the files needed to run DIYABC :
\begin{enumerate}
    \item \textsf{general} : the DIYABC executable
    \item \textsf{header.txt} : the header file
    \item \textsf{launch.sh} : the main script to run
    \item \textsf{node.sh} : the script that will be runned by your scheduler for each job
    \item \textsf{$<yourData.mss>$} : the data file 
\end{enumerate}
Now, you have to run the main script \texttt{launch.sh}. \texttt{launch.sh} will run node.sh for each job

\subsubsection{launch.sh}

Once you are in the $<yourTarArchiveName>$ directory, you can run \texttt{launch.sh}. For instance, for a total of 50,000 data sets to be produced through 5 jobs of 10,000 data sets:\\
%\fbox{%
   \begin{minipage}{0.99\textwidth}
 
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
 
\lstset{ 
  language=bash,                % the language of the code
  basicstyle=\footnotesize,           % the size of the fonts that are used for the code
  numbers=left,                   % where to put the line-numbers
  numberstyle=\tiny\color{gray},  % the style that is used for the line-numbers
  stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  tabsize=4,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
  title=\lstname,                   % show the filename of files included with \lstinputlisting;
                                  % also try caption instead of title
  keywordstyle=\color{black},          % keyword style
  commentstyle=\color{dkgreen},       % comment style
  stringstyle=\color{mauve},         % string literal style
  escapeinside={\%*}{*)},            % if you want to add LaTeX within your code
  morekeywords={*},              % if you want to add more keywords to the set
  deletekeywords={}              % if you want to delete keywords from the given language
}


\begin{lstlisting}
>launch.sh
** Generation of RNG files :
./general -p ./ -n "t:1;c:5;s:1038"

** jobs submition :

qsub -N n1_test -q short.q -cwd node.sh 10000 /home/dehneg/DIYABCtest 1  test.mss
Your job 111598 ("n1_test") has been submitted

qsub -N n2_test -q short.q -cwd node.sh 10000 /home/dehneg/DIYABCtest 2  test.mss
Your job 111599 ("n2_test") has been submitted

qsub -N n3_test -q short.q -cwd node.sh 10000 /home/dehneg/DIYABCtest 3  test.mss
Your job 111600 ("n3_test") has been submitted

qsub -N n4_test -q short.q -cwd node.sh 10000 /home/dehneg/DIYABCtest 4  test.mss
Your job 111601 ("n4_test") has been submitted

qsub -N n5_test -q short.q -cwd node.sh 10000 /home/dehneg/DIYABCtest 5  test.mss
Your job 111602 ("n5_test") has been submitted

** monitoring :
0/5 finished 0%  (total : 0)
1/5 finished 20% (total : 10000)
1/5 finished 20% (total : 10000)
2/5 finished 40% (total : 20000)
4/5 finished 80% (total : 40000)
5/5 finished 100% (total : 50000)

** reftables concatenation  :
./general -p /home/dehneg/DIYABCtest -q 2>&1 concat.out
*************************************************************
All the result files have been concatenated into reftable.bin
See concat.out output file for logs
*************************************************************
\end{lstlisting}
   \end{minipage}
%}
\begin{enumerate}
    \item line 3 : generation of all RNG files : \textsf{RNG\_state\_0000.bin}, ..., \textsf{RNG\_state\_0005.bin}
    \item lines 7, 10, 13, 16, 19 : job submission 
    \item lines 23 to 28 : jobs status given every 30 seconds
    \item line 31 : merge all \textsf{reftable\_$<number>$.bin} file in one reftable file \textsf{reftable.bin}. In case of any problem, please read \textsf{concat.out} output file.
\end{enumerate}
Once the monitoring phase starts, you can quit \texttt{launch.sh} and restart it at any time. Since all jobs are submitted by \texttt{launch.sh}, a lock file named \texttt{launch.sh.lock} is written in your $yourTarArchiveName$ directory and inform any new execution of \texttt{launch.sh} to not submit any new job.

\subsubsection{node.sh}
\texttt{node.sh runs} on your cluster nodes for each jobs. Those are the sequential steps of \texttt{node.sh} :
\begin{enumerate}
    \item create a $job id$ name according to the following pattern : \texttt{<node hostname>-n-<sequential number of the job>-pid<pid of nodes.sh execution>-<a random number>} ($pid$ mean Process IDentifier).
    \item Use the scheduler temporary directory if the scheduler provide a \texttt{TMPDIR} environment variable or create a working temporary directory \texttt{/tmp/tmpDiyabc\_<job id>} on the cluster node.
    \item Choose a RNG file from the pool of RNG files created by \texttt{launch.sh}. It means that the node must access your diyabc $yourTarArchiveName$ directory in your working directory. 
    \item As long as \texttt{node.sh} is using the choosen RNG file, you won't see the RNG file in the $yourTarArchiveName$ directory but a lock file named \texttt{<the choosen RNG file name>}.lock and a flag file named the \texttt{<choosen RNG file name>\_<date of the run>\_<job id>}. The flag file contain the local pid of the job on the node of DIYABC. Once DIYABC has finished and updated the RNG file, you will see it again, but not the lock and flag files.
    \item run DIYABC (general file) of course !
    \item copy periodically the reftable log file to the $yourTarArchiveName$ directory. Thus, \texttt{launch.sh} can inform you of the status of the total dataset computed.
\end{enumerate}



\subsection{transfer back the reference table and include it into your computer project}\label{clusterback}
Once your final \texttt{reftable.bin} file has been produced, you need to transfer it from the cluster to you DIYABC project directory on your own computer. Once again, you can use an sftp client.


\clearpage
\subsection{A note about the random number generator used in DIYABC}\label{rng}
\input{RNG.tex}


  
